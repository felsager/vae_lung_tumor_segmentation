{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation of Medical Scans using Variational VAE's - Part 3/3\n",
    "This series of notebooks enables reproduceability of our final models and testing results.\n",
    "\n",
    "The third notebook goes through the process of creating, training and tuning a variational decoder, which will act as a segmenter.\n",
    "\n",
    "We import some necessary libraries, and check if our GPU is available, while also retrieving some system stats. We need a lot of RAM, because our selected datasets are very large. We setup up some global constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dfels\\anaconda3\\envs\\torch_environment\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n",
      "NVIDIA GeForce RTX 2060 with Max-Q Design\n",
      "CUDA version: 11.7\n",
      "RAM: 33.74GB\n"
     ]
    }
   ],
   "source": [
    "# For ML\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as D\n",
    "import torch.optim as optim\n",
    "from torch import Tensor\n",
    "\n",
    "# For displaying and evaluating results.\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# For monitoring resource-usage and progress.\n",
    "from timeit import default_timer as timer\n",
    "import psutil\n",
    "from os.path import join\n",
    "\n",
    "# Our own utility functions, constants and classes.\n",
    "from utility import CT_Dataset, superimpose, draw\n",
    "\n",
    "# Our own DL models.\n",
    "from models import VAEModel, SegmentationModel, Conv, ConvTranspose\n",
    "\n",
    "# Paths.\n",
    "root_dir = '../' # Relative to the working directory.\n",
    "raw_data_dir = join(root_dir, 'raw_data')\n",
    "prep_data_dir = join(root_dir, 'prep_data')\n",
    "losses_dir = join(root_dir, 'losses')\n",
    "models_dir = join(root_dir, 'saved_models')\n",
    "checkpoint_dir = join(root_dir, 'checkpoints')\n",
    "\n",
    "\n",
    "# Setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using', device)\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('CUDA version:', torch.version.cuda)\n",
    "\n",
    "available_ram = round(psutil.virtual_memory()[0]/1000000000,2)\n",
    "print('RAM: ' + str(available_ram) + 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a new function to create dataloaders - This time splitting the dataset into a training-, development- and testing-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_seg_loaders(data, batch_size):\n",
    "    N = len(data); N_train = int(0.8*N); \n",
    "    N_dev = int((N - N_train)/2); N_test = int(N - N_train - N_dev)\n",
    "    train_data, dev_data, test_data = D.random_split(data, [N_train, N_dev, N_test])\n",
    "    train_loader = D.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    dev_loader = D.DataLoader(dev_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = D.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "    return train_loader, dev_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we create an instance of the full dataset, and pass it the dataloader creator. Naturally, we use the same resolution and batch-size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = 2**8; batch_size = 32;\n",
    "dataset = CT_Dataset(prep_data_dir, 'lung', resolution)\n",
    "train_loader, dev_loader, test_loader = make_seg_loaders(dataset, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we simply use a binary-cross-entropy loss-function.\n",
    "\n",
    "We also define a training-routine for a single epoch, i.e. a full round of training data, as well as an evaluation routine. These routines are different from the routines in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss(reduction='sum') # reduction='sum' to make tumors more important.\n",
    "\n",
    "def train_epoch(vae_model, seg_model, optimizer, train_loader):\n",
    "    seg_model.train()\n",
    "    vae_model.eval()\n",
    "    losses = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        x = batch[0].to(device) # Batch of images.\n",
    "        y = batch[1].to(device) # Batch of labels.\n",
    "        optimizer.zero_grad()\n",
    "        z = vae_model.forward_latent(x) # Get latent vector from VAE.\n",
    "        y_hat = seg_model.forward(z) # Reconstruction from new decoder.\n",
    "        loss = loss_fn(y_hat, y) # Compare reconstruction to label.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item()   \n",
    "    \n",
    "    return losses / len(train_loader)  # average loss\n",
    "\n",
    "def evaluate(vae_model, seg_model, dev_loader):\n",
    "    seg_model.eval()\n",
    "    vae_model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    for data in dev_loader:\n",
    "        x = data[0].to(device)\n",
    "        y = data[1].to(device)\n",
    "        z = vae_model.forward_latent(x)\n",
    "        y_hat = seg_model.forward(z)\n",
    "        loss = loss_fn(y_hat, y)\n",
    "        losses += loss.item() \n",
    "    return losses / len(dev_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load in our trained model, which we created in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_model = torch.load(join(models_dir, 'vae_model.pt')).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an untrained instance of our `SegmentationModel` class, which we have defined in the `models.py` file. We specify an optimizer, as well as a learning rate scheduler. Again, we found that the `AdamW` optimizer, which is Adam with weight-decay, works slightly better than regular `Adam`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_model = SegmentationModel(base=16).to(device)\n",
    "\n",
    "optimizer = optim.AdamW(seg_model.parameters(), lr=5e-3)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.6)\n",
    "\n",
    "a = vae_model.state_dict()\n",
    "b = seg_model.state_dict()\n",
    "\n",
    "for key in b:\n",
    "    b[key] = a[key]\n",
    "seg_model.load_state_dict(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a single decoder is a lot faster than a full VAE, so we do not need checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:, Train-loss: 463.7081, Dev-loss: 577.0502, Epoch-time = 58.189s\n",
      "Epoch 2:, Train-loss: 463.0314, Dev-loss: 569.7976, Epoch-time = 58.552s\n",
      "Epoch 3:, Train-loss: 447.0540, Dev-loss: 536.1280, Epoch-time = 58.894s\n",
      "Epoch 4:, Train-loss: 463.6483, Dev-loss: 575.0859, Epoch-time = 58.982s\n",
      "Epoch 5:, Train-loss: 447.7964, Dev-loss: 563.9321, Epoch-time = 59.070s\n",
      "Epoch 6:, Train-loss: 455.6000, Dev-loss: 553.8924, Epoch-time = 59.143s\n",
      "Epoch 7:, Train-loss: 445.1437, Dev-loss: 595.3459, Epoch-time = 59.249s\n",
      "Epoch 8:, Train-loss: 453.2779, Dev-loss: 546.5738, Epoch-time = 59.034s\n",
      "Epoch 9:, Train-loss: 455.5361, Dev-loss: 600.1134, Epoch-time = 59.028s\n",
      "Epoch 10:, Train-loss: 456.6685, Dev-loss: 575.7604, Epoch-time = 59.053s\n",
      "Epoch 11:, Train-loss: 459.5847, Dev-loss: 564.9786, Epoch-time = 59.026s\n",
      "Epoch 12:, Train-loss: 442.9384, Dev-loss: 547.0142, Epoch-time = 59.025s\n",
      "Manually stopped.\n"
     ]
    }
   ],
   "source": [
    "total_epochs = 50\n",
    "\n",
    "#train_losses = []; dev_losses = []; lrs = []\n",
    "\n",
    "try:\n",
    "    for epoch in range(1, total_epochs+1):\n",
    "        lrs.append(optimizer.param_groups[0]['lr'])\n",
    "        start_time = timer()\n",
    "        train_loss = train_epoch(vae_model, seg_model, optimizer, train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        end_time = timer()\n",
    "        dev_loss = evaluate(vae_model, seg_model, dev_loader)\n",
    "        dev_losses.append(dev_loss)\n",
    "        scheduler.step()\n",
    "        \n",
    "        print((f\"Epoch {epoch}:, Train-loss: {train_loss:.4f}, Dev-loss: {dev_loss:.4f}, \"f\"Epoch-time = {(end_time - start_time):.3f}s\"))\n",
    "\n",
    "    print('Training completed.')\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print('Manually stopped.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the trained segmentation model along with loss metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CAREFUL NOT TO OVERWRITE.\n",
    "#torch.save(seg_model, join(models_dir, 'seg_model.pt')) \n",
    "#torch.save(train_losses, join(losses_dir, 'seg_train_losses.pt'))\n",
    "#torch.save(dev_losses, join(losses_dir, 'seg_dev_losses.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_model = torch.load(join(models_dir, 'seg_model.pt')).to(device)\n",
    "train_losses = torch.load(join(losses_dir, 'seg_train_losses.pt'))\n",
    "dev_losses = torch.load(join(losses_dir, 'seg_dev_losses.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''' Segmentation evaluation '''\n",
    "eps = 1e-16\n",
    "def IoU(label, recon, thresh):\n",
    "    inter = ((label >= thresh) & (recon >= thresh)) * 1.0\n",
    "    union = ((label >= thresh) | (recon >= thresh)) * 1.0\n",
    "    return inter.sum() / (union.sum() + eps) / label.shape[0]\n",
    "\n",
    "def seg_evaluate(vae_model, seg_model, loader):\n",
    "    seg_model.eval()\n",
    "    vae_model.eval()\n",
    "    iou = 0\n",
    "    no_labels = 0\n",
    "\n",
    "    for data in loader:\n",
    "        x = data[0].to(device)\n",
    "        y = data[1].to(device)\n",
    "        if y.sum() != 0: # Only evaluate if there is a tumor.\n",
    "            z = vae_model.forward_latent(x)\n",
    "            y_hat = seg_model.forward(z)\n",
    "            iou += IoU(y, y_hat, 0.5)\n",
    "            no_labels += 1\n",
    "    return iou / no_labels # Average IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0213, device='cuda:0')"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_evaluate(vae_model, seg_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tell model to not update weights.\n",
    "from matplotlib.colors import ListedColormap\n",
    "cmap_label = ListedColormap(['none', 'green'])\n",
    "cmap_pred = ListedColormap(['none', 'red'])\n",
    "\n",
    "vae_model.eval()\n",
    "seg_model.eval()\n",
    "\n",
    "image, label = next(iter(dev_loader))\n",
    "\n",
    "x = image[1][None, :].to(device) # Batch of 1 image.\n",
    "y = label[1][None, :].to(device) # Batch of 1 label.\n",
    "\n",
    "z = vae_model.forward_latent(x)\n",
    "y_hat = seg_model.forward(z)\n",
    "\n",
    "truth = torch.squeeze(y.cpu()).detach().numpy()\n",
    "guess = (torch.squeeze(y_hat.cpu()).detach().numpy() > 0.5) * 1.0\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "ax.imshow(truth, cmap='gray', vmin=0, vmax=1)\n",
    "#ax.imshow(guess, cmap='gray', vmin=0, vmax=1)\n",
    "fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(test_loader, '../prep_data/test_loader.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4774e11bc94230fbd552f0cedba07848c67b493bb83ff5140fd93cc0e9d8c643"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
