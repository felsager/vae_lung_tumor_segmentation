{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation of Medical Scans using Variational VAE's - Part 3/3\n",
    "This series of notebooks enables reproduceability of our final models and testing results.\n",
    "\n",
    "The third notebook goes through the process of creating, training and tuning a variational decoder, which will act as a segmenter.\n",
    "\n",
    "We import some necessary libraries, and check if our GPU is available, while also retrieving some system stats. We need a lot of RAM, because our selected datasets are very large. We setup up some global constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dfels\\anaconda3\\envs\\torch_environment\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n",
      "NVIDIA GeForce RTX 2060 with Max-Q Design\n",
      "CUDA version: 11.7\n",
      "RAM: 33.74GB\n"
     ]
    }
   ],
   "source": [
    "# For ML\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as D\n",
    "import torch.optim as optim\n",
    "from torch import Tensor\n",
    "\n",
    "# For displaying and evaluating results.\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# For monitoring resource-usage and progress.\n",
    "from timeit import default_timer as timer\n",
    "import psutil\n",
    "from os.path import join\n",
    "\n",
    "# Our own utility functions, constants and classes.\n",
    "from utility import CT_Dataset, superimpose, draw\n",
    "\n",
    "# Our own DL models.\n",
    "from models import VAEModel, SegmentationModel, Conv, ConvTranspose\n",
    "\n",
    "# Paths.\n",
    "root_dir = '../' # Relative to the working directory.\n",
    "raw_data_dir = join(root_dir, 'raw_data')\n",
    "prep_data_dir = join(root_dir, 'prep_data')\n",
    "losses_dir = join(root_dir, 'losses')\n",
    "models_dir = join(root_dir, 'saved_models')\n",
    "checkpoint_dir = join(root_dir, 'checkpoints')\n",
    "\n",
    "\n",
    "# Setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using', device)\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('CUDA version:', torch.version.cuda)\n",
    "\n",
    "available_ram = round(psutil.virtual_memory()[0]/1000000000,2)\n",
    "print('RAM: ' + str(available_ram) + 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a new function to create dataloaders - This time splitting the dataset into a training-, development- and testing-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_seg_loaders(data, batch_size):\n",
    "    N = len(data); N_train = int(0.8*N); \n",
    "    N_dev = int((N - N_train)/2); N_test = int(N - N_train - N_dev)\n",
    "    train_data, dev_data, test_data = D.random_split(data, [N_train, N_dev, N_test])\n",
    "    train_loader = D.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    dev_loader = D.DataLoader(dev_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = D.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "    return train_loader, dev_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we create an instance of the full dataset, and pass it the dataloader creator. Naturally, we use the same resolution and batch-size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = 2**8; batch_size = 32;\n",
    "dataset = CT_Dataset(prep_data_dir, 'lung', resolution)\n",
    "train_loader, dev_loader, test_loader = make_seg_loaders(dataset, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we simply use a binary-cross-entropy loss-function.\n",
    "\n",
    "We also define a training-routine for a single epoch, i.e. a full round of training data, as well as an evaluation routine. These routines are different from the routines in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss(reduction='sum') # reduction='sum' to make tumors more important.\n",
    "\n",
    "def train_epoch(vae_model, seg_model, optimizer, train_loader):\n",
    "    seg_model.train()\n",
    "    vae_model.eval()\n",
    "    losses = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        x = batch[0].to(device) # Batch of images.\n",
    "        y = batch[1].to(device) # Batch of labels.\n",
    "        optimizer.zero_grad()\n",
    "        z = vae_model.forward_latent(x) # Get latent vector from VAE.\n",
    "        y_hat = seg_model.forward(z) # Reconstruction from new decoder.\n",
    "        loss = loss_fn(y_hat, y) # Compare reconstruction to label.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item()   \n",
    "    \n",
    "    return losses / len(train_loader)  # average loss\n",
    "\n",
    "def evaluate(vae_model, seg_model, dev_loader):\n",
    "    seg_model.eval()\n",
    "    vae_model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    for data in dev_loader:\n",
    "        x = data[0].to(device)\n",
    "        y = data[1].to(device)\n",
    "        z = vae_model.forward_latent(x)\n",
    "        y_hat = seg_model.forward(z)\n",
    "        loss = loss_fn(y_hat, y)\n",
    "        losses += loss.item() \n",
    "    return losses / len(dev_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load in our trained model, which we created in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_model = torch.load(join(models_dir, 'vae_model.pt')).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an untrained instance of our `SegmentationModel` class, which we have defined in the `models.py` file. We specify an optimizer, as well as a learning rate scheduler. Again, we found that the `AdamW` optimizer, which is Adam with weight-decay, works slightly better than regular `Adam`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_model = SegmentationModel(base=16).to(device)\n",
    "\n",
    "optimizer = optim.AdamW(seg_model.parameters(), lr=5e-3)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.6)\n",
    "\n",
    "a = vae_model.state_dict()\n",
    "b = seg_model.state_dict()\n",
    "\n",
    "for key in b:\n",
    "    b[key] = a[key]\n",
    "seg_model.load_state_dict(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a single decoder is a lot faster than a full VAE, so we do not need checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:, Train-loss: 463.7081, Dev-loss: 577.0502, Epoch-time = 58.189s\n",
      "Epoch 2:, Train-loss: 463.0314, Dev-loss: 569.7976, Epoch-time = 58.552s\n",
      "Epoch 3:, Train-loss: 447.0540, Dev-loss: 536.1280, Epoch-time = 58.894s\n",
      "Epoch 4:, Train-loss: 463.6483, Dev-loss: 575.0859, Epoch-time = 58.982s\n",
      "Epoch 5:, Train-loss: 447.7964, Dev-loss: 563.9321, Epoch-time = 59.070s\n",
      "Epoch 6:, Train-loss: 455.6000, Dev-loss: 553.8924, Epoch-time = 59.143s\n",
      "Epoch 7:, Train-loss: 445.1437, Dev-loss: 595.3459, Epoch-time = 59.249s\n",
      "Epoch 8:, Train-loss: 453.2779, Dev-loss: 546.5738, Epoch-time = 59.034s\n",
      "Epoch 9:, Train-loss: 455.5361, Dev-loss: 600.1134, Epoch-time = 59.028s\n",
      "Epoch 10:, Train-loss: 456.6685, Dev-loss: 575.7604, Epoch-time = 59.053s\n",
      "Epoch 11:, Train-loss: 459.5847, Dev-loss: 564.9786, Epoch-time = 59.026s\n",
      "Epoch 12:, Train-loss: 442.9384, Dev-loss: 547.0142, Epoch-time = 59.025s\n",
      "Manually stopped.\n"
     ]
    }
   ],
   "source": [
    "total_epochs = 50\n",
    "\n",
    "#train_losses = []; dev_losses = []; lrs = []\n",
    "\n",
    "try:\n",
    "    for epoch in range(1, total_epochs+1):\n",
    "        lrs.append(optimizer.param_groups[0]['lr'])\n",
    "        start_time = timer()\n",
    "        train_loss = train_epoch(vae_model, seg_model, optimizer, train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        end_time = timer()\n",
    "        dev_loss = evaluate(vae_model, seg_model, dev_loader)\n",
    "        dev_losses.append(dev_loss)\n",
    "        scheduler.step()\n",
    "        \n",
    "        print((f\"Epoch {epoch}:, Train-loss: {train_loss:.4f}, Dev-loss: {dev_loss:.4f}, \"f\"Epoch-time = {(end_time - start_time):.3f}s\"))\n",
    "\n",
    "    print('Training completed.')\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print('Manually stopped.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the trained segmentation model along with loss metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CAREFUL NOT TO OVERWRITE.\n",
    "#torch.save(seg_model, join(models_dir, 'seg_model.pt')) \n",
    "#torch.save(train_losses, join(losses_dir, 'seg_train_losses.pt'))\n",
    "#torch.save(dev_losses, join(losses_dir, 'seg_dev_losses.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_model = torch.load(join(models_dir, 'seg_model.pt')).to(device)\n",
    "train_losses = torch.load(join(losses_dir, 'seg_train_losses.pt'))\n",
    "dev_losses = torch.load(join(losses_dir, 'seg_dev_losses.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''' Segmentation evaluation '''\n",
    "eps = 1e-16\n",
    "def IoU(label, recon, thresh):\n",
    "    inter = ((label >= thresh) & (recon >= thresh)) * 1.0\n",
    "    union = ((label >= thresh) | (recon >= thresh)) * 1.0\n",
    "    return inter.sum() / (union.sum() + eps) / label.shape[0]\n",
    "\n",
    "def seg_evaluate(vae_model, seg_model, loader):\n",
    "    seg_model.eval()\n",
    "    vae_model.eval()\n",
    "    iou = 0\n",
    "    no_labels = 0\n",
    "\n",
    "    for data in loader:\n",
    "        x = data[0].to(device)\n",
    "        y = data[1].to(device)\n",
    "        if y.sum() != 0: # Only evaluate if there is a tumor.\n",
    "            z = vae_model.forward_latent(x)\n",
    "            y_hat = seg_model.forward(z)\n",
    "            iou += IoU(y, y_hat, 0.5)\n",
    "            no_labels += 1\n",
    "    return iou / no_labels # Average IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No tumor in this image.\n",
      "No tumor in this image.\n",
      "No tumor in this image.\n",
      "No tumor in this image.\n",
      "No tumor in this image.\n",
      "No tumor in this image.\n",
      "No tumor in this image.\n",
      "No tumor in this image.\n",
      "No tumor in this image.\n",
      "No tumor in this image.\n",
      "No tumor in this image.\n",
      "No tumor in this image.\n",
      "No tumor in this image.\n",
      "No tumor in this image.\n",
      "No tumor in this image.\n",
      "No tumor in this image.\n",
      "No tumor in this image.\n",
      "No tumor in this image.\n",
      "No tumor in this image.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0226, device='cuda:0')"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_evaluate(vae_model, seg_model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tell model to not update weights.\n",
    "vae_model.eval()\n",
    "seg_model.eval()\n",
    "\n",
    "image, label = next(iter(dev_loader))\n",
    "\n",
    "x = image[1][None, :].to(device) # Batch of 1 image.\n",
    "y = label[1][None, :].to(device) # Batch of 1 label.\n",
    "\n",
    "z = vae_model.forward_latent(x)\n",
    "y_hat = seg_model.forward(z)\n",
    "\n",
    "truth = torch.squeeze(y.cpu()).detach().numpy()\n",
    "guess = (torch.squeeze(y_hat.cpu()).detach().numpy() > 0.5) * 1.0\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8,5))\n",
    "axs[0].imshow(truth,cmap='gray')\n",
    "axs[1].imshow(guess,cmap='gray')\n",
    "fig.canvas.draw()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4774e11bc94230fbd552f0cedba07848c67b493bb83ff5140fd93cc0e9d8c643"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
