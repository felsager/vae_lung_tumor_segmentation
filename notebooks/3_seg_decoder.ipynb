{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation of Medical Scans using Variational VAE's - Part 3/3\n",
    "This series of notebooks enables reproduceability of our final models and testing results.\n",
    "\n",
    "The third notebook goes through the process of creating, training and tuning a variational decoder, which will act as a segmenter.\n",
    "\n",
    "We import some necessary libraries, and check if our GPU is available, while also retrieving some system stats. We need a lot of RAM, because our selected datasets are very large. We setup up some global constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dfels\\anaconda3\\envs\\torch_environment\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n",
      "NVIDIA GeForce RTX 2060 with Max-Q Design\n",
      "CUDA version: 11.7\n",
      "RAM: 33.74GB\n"
     ]
    }
   ],
   "source": [
    "# For ML\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as D\n",
    "import torch.optim as optim\n",
    "from torch import Tensor\n",
    "\n",
    "# For displaying and evaluating results.\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# For monitoring resource-usage and progress.\n",
    "from timeit import default_timer as timer\n",
    "import psutil\n",
    "from os.path import join\n",
    "\n",
    "# Our own utility functions, constants and classes.\n",
    "from utility import CT_Dataset, superimpose, draw\n",
    "\n",
    "# Our own DL models.\n",
    "from models import VAEModel, SegmentationModel, Conv, ConvTranspose\n",
    "\n",
    "# Paths.\n",
    "root_dir = '../' # Relative to the working directory.\n",
    "raw_data_dir = join(root_dir, 'raw_data')\n",
    "prep_data_dir = join(root_dir, 'prep_data')\n",
    "losses_dir = join(root_dir, 'losses')\n",
    "models_dir = join(root_dir, 'saved_models')\n",
    "checkpoint_dir = join(root_dir, 'checkpoints')\n",
    "\n",
    "\n",
    "# Setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using', device)\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('CUDA version:', torch.version.cuda)\n",
    "\n",
    "available_ram = round(psutil.virtual_memory()[0]/1000000000,2)\n",
    "print('RAM: ' + str(available_ram) + 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a new function to create dataloaders - This time splitting the dataset into a training-, development- and testing-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_seg_loaders(data, batch_size):\n",
    "    N = len(data); N_train = int(0.8*N); \n",
    "    N_dev = int((N - N_train)/2); N_test = int(N - N_train - N_dev)\n",
    "    train_data, dev_data, test_data = D.random_split(data, [N_train, N_dev, N_test])\n",
    "    train_loader = D.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    dev_loader = D.DataLoader(dev_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = D.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "    return train_loader, dev_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we create an instance of the full dataset, and pass it the dataloader creator. Naturally, we use the same resolution and batch-size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = 2**8; batch_size = 32;\n",
    "dataset = CT_Dataset(prep_data_dir, 'lung', resolution)\n",
    "train_loader, dev_loader, test_loader = make_seg_loaders(dataset, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we simply use a binary-cross-entropy loss-function.\n",
    "\n",
    "We also define a training-routine for a single epoch, i.e. a full round of training data, as well as an evaluation routine. These routines are different from the routines in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss(reduction='sum') # reduction='sum' to make tumors more important.\n",
    "\n",
    "def train_epoch(vae_model, seg_model, optimizer, train_loader):\n",
    "    seg_model.train()\n",
    "    vae_model.eval()\n",
    "    losses = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        x = batch[0].to(device) # Batch of images.\n",
    "        y = batch[1].to(device) # Batch of labels.\n",
    "        optimizer.zero_grad()\n",
    "        z = vae_model.forward_latent(x) # Get latent vector from VAE.\n",
    "        y_hat = seg_model.forward(z) # Reconstruction from new decoder.\n",
    "        loss = loss_fn(y_hat, y) # Compare reconstruction to label.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item()   \n",
    "    \n",
    "    return losses / len(train_loader)  # average loss\n",
    "\n",
    "def evaluate(vae_model, seg_model, dev_loader):\n",
    "    seg_model.eval()\n",
    "    vae_model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    for data in dev_loader:\n",
    "        x = data[0].to(device)\n",
    "        y = data[1].to(device)\n",
    "        z = vae_model.forward_latent(x)\n",
    "        y_hat = seg_model.forward(z)\n",
    "        loss = loss_fn(y_hat, y)\n",
    "        losses += loss.item() \n",
    "    return losses / len(dev_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load in our trained model, which we created in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_model = torch.load(join(models_dir, 'vae_model.pt')).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an untrained instance of our `SegmentationModel` class, which we have defined in the `models.py` file. We specify an optimizer, as well as a learning rate scheduler. Again, we found that the `AdamW` optimizer, which is Adam with weight-decay, works slightly better than regular `Adam`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_model = SegmentationModel(base=16).to(device)\n",
    "\n",
    "optimizer = optim.AdamW(seg_model.parameters(), lr=5e-3)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.6)\n",
    "\n",
    "a = vae_model.state_dict()\n",
    "b = seg_model.state_dict()\n",
    "\n",
    "for key in b:\n",
    "    b[key] = a[key]\n",
    "seg_model.load_state_dict(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a single decoder is a lot faster than a full VAE, so we do not need checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:, Train-loss: 605.8504, Dev-loss: 688.1402, Epoch-time = 57.958s\n",
      "Epoch 2:, Train-loss: 576.4299, Dev-loss: 640.0578, Epoch-time = 58.315s\n",
      "Epoch 3:, Train-loss: 579.5166, Dev-loss: 660.3075, Epoch-time = 58.527s\n",
      "Epoch 4:, Train-loss: 558.3265, Dev-loss: 682.7208, Epoch-time = 58.659s\n",
      "Epoch 5:, Train-loss: 566.4939, Dev-loss: 664.7597, Epoch-time = 58.751s\n",
      "Epoch 6:, Train-loss: 572.4855, Dev-loss: 642.2945, Epoch-time = 58.785s\n",
      "Epoch 7:, Train-loss: 556.4792, Dev-loss: 661.4580, Epoch-time = 58.837s\n",
      "Epoch 8:, Train-loss: 555.8911, Dev-loss: 618.3747, Epoch-time = 58.842s\n",
      "Epoch 9:, Train-loss: 546.9838, Dev-loss: 603.4302, Epoch-time = 58.842s\n",
      "Epoch 10:, Train-loss: 538.4515, Dev-loss: 675.1134, Epoch-time = 58.990s\n",
      "Epoch 11:, Train-loss: 531.5901, Dev-loss: 643.6776, Epoch-time = 59.058s\n",
      "Epoch 12:, Train-loss: 529.8058, Dev-loss: 588.1274, Epoch-time = 59.195s\n",
      "Epoch 13:, Train-loss: 529.3146, Dev-loss: 592.0153, Epoch-time = 59.103s\n",
      "Epoch 14:, Train-loss: 529.3587, Dev-loss: 604.1970, Epoch-time = 59.047s\n",
      "Epoch 15:, Train-loss: 517.5323, Dev-loss: 581.5042, Epoch-time = 59.196s\n",
      "Epoch 16:, Train-loss: 526.8024, Dev-loss: 592.9754, Epoch-time = 59.185s\n",
      "Epoch 17:, Train-loss: 510.0929, Dev-loss: 604.2995, Epoch-time = 59.207s\n",
      "Epoch 18:, Train-loss: 505.4207, Dev-loss: 601.5986, Epoch-time = 59.321s\n",
      "Epoch 19:, Train-loss: 505.7837, Dev-loss: 648.7453, Epoch-time = 59.482s\n",
      "Epoch 20:, Train-loss: 501.7105, Dev-loss: 631.8115, Epoch-time = 59.196s\n",
      "Epoch 21:, Train-loss: 507.0745, Dev-loss: 599.8016, Epoch-time = 59.257s\n",
      "Epoch 22:, Train-loss: 497.6507, Dev-loss: 630.1344, Epoch-time = 59.167s\n",
      "Epoch 23:, Train-loss: 496.6423, Dev-loss: 569.3547, Epoch-time = 59.276s\n",
      "Epoch 24:, Train-loss: 488.9052, Dev-loss: 561.7597, Epoch-time = 59.276s\n",
      "Epoch 25:, Train-loss: 486.1016, Dev-loss: 594.5082, Epoch-time = 59.298s\n",
      "Epoch 26:, Train-loss: 489.7388, Dev-loss: 579.5713, Epoch-time = 59.253s\n",
      "Epoch 27:, Train-loss: 488.3878, Dev-loss: 597.3146, Epoch-time = 59.207s\n"
     ]
    }
   ],
   "source": [
    "total_epochs = 50\n",
    "\n",
    "#train_losses = []; dev_losses = []; lrs = []\n",
    "\n",
    "try:\n",
    "    for epoch in range(1, total_epochs+1):\n",
    "        lrs.append(optimizer.param_groups[0]['lr'])\n",
    "        start_time = timer()\n",
    "        train_loss = train_epoch(vae_model, seg_model, optimizer, train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        end_time = timer()\n",
    "        dev_loss = evaluate(vae_model, seg_model, dev_loader)\n",
    "        dev_losses.append(dev_loss)\n",
    "        scheduler.step()\n",
    "        \n",
    "        print((f\"Epoch {epoch}:, Train-loss: {train_loss:.4f}, Dev-loss: {dev_loss:.4f}, \"f\"Epoch-time = {(end_time - start_time):.3f}s\"))\n",
    "\n",
    "    print('Training completed.')\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print('Manually stopped.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the trained segmentation model along with loss metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CAREFUL NOT TO OVERWRITE.\n",
    "torch.save(seg_model, join(models_dir, 'seg_model.pt')) \n",
    "torch.save(train_losses, join(losses_dir, 'seg_train_losses.pt'))\n",
    "torch.save(dev_losses, join(losses_dir, 'seg_dev_losses.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''' Segmentation evaluation '''\n",
    "eps = 1e-16\n",
    "def IoU(label, recon, thresh):\n",
    "    inter = ((label >= thresh) & (recon >= thresh)) * 1.0\n",
    "    union = ((label >= thresh) | (recon >= thresh)) * 1.0\n",
    "    return inter.sum() / (union.sum() + eps) / label.shape[0]\n",
    "\n",
    "def seg_evaluate(vae_model, seg_model, loader):\n",
    "    seg_model.eval()\n",
    "    vae_model.eval()\n",
    "    iou = 0\n",
    "\n",
    "    for data in loader:\n",
    "        x = data[0].to(device)\n",
    "        y = data[1].to(device)\n",
    "        z = vae_model.forward_latent(x)\n",
    "        y_hat = seg_model.forward(z)\n",
    "        iou += IoU(y, y_hat, 0.5)\n",
    "    return iou / len(test_loader) # Average IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0079, device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_evaluate(vae_model, seg_model, dev_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tell model to not update weights.\n",
    "vae_model.eval()\n",
    "seg_model.eval()\n",
    "\n",
    "image, label = next(iter(train_loader))\n",
    "\n",
    "x = image[1][None, :].to(device) # Batch of 1 image.\n",
    "y = label[1][None, :].to(device) # Batch of 1 label.\n",
    "\n",
    "z = vae_model.forward_latent(x)\n",
    "y_hat = seg_model.forward(z)\n",
    "\n",
    "truth = torch.squeeze(y.cpu()).detach().numpy()\n",
    "guess = torch.squeeze(y_hat.cpu()).detach().numpy()\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8,5))\n",
    "axs[0].imshow(truth,cmap='gray')\n",
    "axs[1].imshow(guess,cmap='gray')\n",
    "fig.canvas.draw()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4774e11bc94230fbd552f0cedba07848c67b493bb83ff5140fd93cc0e9d8c643"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
