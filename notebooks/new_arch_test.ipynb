{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHECK README"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation of Medical Scans using Variational VAE's\n",
    "This notebook goes through the process of creating a pytorch-compatible dataset, and setting up a model for segmentation of tumors in various organs. It enables reproduceability of our final model and testing results.\n",
    "\n",
    "## 1. Setup\n",
    "We import some necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For ML\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as D\n",
    "import torch.optim as optim\n",
    "from torch import Tensor\n",
    "import torchvision.transforms as Transform\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# For reading raw data.\n",
    "import json\n",
    "import nibabel as nib\n",
    "\n",
    "# For displaying and evaluating results.\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# For monitoring resource-usage and progress.\n",
    "from timeit import default_timer as timer\n",
    "from tqdm import tqdm # Install ipywidgets to remove warning.\n",
    "import os, sys, psutil\n",
    "from os.path import join, exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And check if our GPU is available, while also retrieving some system stats. We need a lot of RAM, because our selected datasets are very large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n",
      "NVIDIA GeForce GTX 1070\n",
      "CUDA version: 11.7\n",
      "RAM: 16.74GB\n"
     ]
    }
   ],
   "source": [
    "# Setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using', device)\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('CUDA version:', torch.version.cuda)\n",
    "\n",
    "available_ram = round(psutil.virtual_memory()[0]/1000000000,2)\n",
    "print('RAM: ' + str(available_ram) + 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We setup up some global constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '../' # Location of project, relative to the working directory.\n",
    "raw_data_dir = join(root_dir, 'raw_data')\n",
    "prep_data_dir = join(root_dir, 'prep_data')\n",
    "losses_dir = join(root_dir, 'losses')\n",
    "models_dir = join(root_dir, 'saved_models')\n",
    "checkpoint_dir = join(root_dir, 'checkpoints')\n",
    "cmap_seg = ListedColormap(['none', 'red']) # For drawing tumors in red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And some utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def superimpose(image, label):\n",
    "    plt.imshow(torch.squeeze(image), cmap='gray')\n",
    "    plt.imshow(torch.squeeze(image), cmap=cmap_seg)\n",
    "    plt.show()\n",
    "\n",
    "def draw(x, x_hat):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(8,5))\n",
    "    img_0 = x[0].detach().numpy()\n",
    "    img_1 = x_hat[0].detach().numpy()\n",
    "    axs[0].imshow(img_0, vmin=0, vmax=1, cmap='gray')\n",
    "    axs[1].imshow(img_1, vmin=0, vmax=1, cmap='gray')\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_checkpoints(dir):\n",
    "    epochs = []\n",
    "    for name in os.listdir(dir):\n",
    "        if os.path.splitext(name)[-1] == '.pth':\n",
    "            epochs += [int(name.strip('ckpt_.pth'))]\n",
    "    return epochs\n",
    "\n",
    "def save_checkpoint(dir, epoch, model, optimizer=None):\n",
    "    checkpoint = {}; checkpoint['epoch'] = epoch\n",
    "\n",
    "    if isinstance(model, torch.nn.DataParallel):\n",
    "        checkpoint['model'] = model.module.state_dict()\n",
    "    else:\n",
    "        checkpoint['model'] = model.state_dict()\n",
    "\n",
    "    if optimizer is not None:\n",
    "        checkpoint['optimizer'] = optimizer.state_dict()\n",
    "    else:\n",
    "        checkpoint['optimizer'] = None\n",
    "\n",
    "    torch.save(checkpoint, os.path.join(dir, 'ckpt_%02d.pth'% epoch))\n",
    "\n",
    "def load_checkpoint(dir, epoch=0):\n",
    "    if epoch == 0: epoch = max(list_checkpoints(dir))\n",
    "    checkpoint_path = os.path.join(dir, 'ckpt_%02d.pth'% epoch)\n",
    "    return torch.load(checkpoint_path, map_location='cpu')\n",
    "\n",
    "def load_model(dir, model, epoch=0):\n",
    "    ckpt = load_checkpoint(dir, epoch)\n",
    "    if isinstance(model, torch.nn.DataParallel):\n",
    "        model.module.load_state_dict(ckpt['model'])\n",
    "    else:\n",
    "        model.load_state_dict(ckpt['model'])\n",
    "    return model\n",
    "\n",
    "def load_optimizer(dir, optimizer, epoch=0):\n",
    "    ckpt = load_checkpoint(dir, epoch)\n",
    "    optimizer.load_state_dict(ckpt['optimizer'])\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We preprocess and format datasets from raw data for each specified organ, using the above function. We save progress after each organ is completed. Can be interrupted and resumed at any time, and accounts for progress, which has already been made. We define a function which loads and stores our data in the proper formatting. As the datasets are huge and have to concatenate each set of 240 slices to the previous, we monitor progress and RAM-usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(organ, type, resolution):\n",
    "    with open(join(raw_data_dir, organ, 'dataset.json')) as f:\n",
    "        manifest = json.load(f)['training']\n",
    "    \n",
    "    bar = tqdm(total=len(manifest))\n",
    "    bar.set_description('Prepping ' + organ + ' ' + type + 's')\n",
    "    \n",
    "    resize = Transform.Resize((resolution, resolution))\n",
    "\n",
    "    try: \n",
    "        images = torch.zeros((0, resolution, resolution))\n",
    "\n",
    "        for entry in manifest:\n",
    "            bar.set_postfix(**{'RAM':round(psutil.virtual_memory()[3]/10e8, 2)})\n",
    "            bar.update()\n",
    "\n",
    "            nii_img = nib.load(join(raw_data_dir, organ, entry[type][2:]))\n",
    "\n",
    "            # Convert to numpy array, then pytorch tensor.\n",
    "            nii_data = Tensor(nii_img.get_fdata())\n",
    "            \n",
    "            # Scale between 0 and 1. Try -1 and 1?\n",
    "            nii_data -= nii_data.min()\n",
    "            nii_data /= nii_data.max()\n",
    "\n",
    "            nii_data = nii_data.permute(2, 0, 1) # (slice, rows, columns)\n",
    "            nii_data = resize(nii_data)\n",
    "            images = torch.cat((images, nii_data), 0)\n",
    "        \n",
    "        torch.save(images, join(prep_data_dir, organ + '_' + type + '_slices_' + str(resolution) + '.pt'))\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print('Manually stopped.')\n",
    "    \n",
    "    bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call the preprocessor function for the organs, we wish to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lod = 2**8                 # Level of detail.\n",
    "resolution = lod           # 2**8 = 256\n",
    "do_prep = False            # Toggle to prep data.\n",
    "\n",
    "organs = ['spleen'] #,'colon','pancreas','lung','liver']\n",
    "\n",
    "if do_prep:\n",
    "    for organ in organs:\n",
    "        prep_data(organ,'image',resolution)\n",
    "        prep_data(organ,'label',resolution)\n",
    "else:\n",
    "    print('Data already prepped.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Managing data\n",
    "We define a custom dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CT_Dataset(Dataset):\n",
    "    def __init__(self, path, organ, resolution):\n",
    "        self.images = torch.load(join(path, organ + '_image_slices_' + str(resolution) + '.pt'))\n",
    "        \n",
    "        self.labels = torch.load(join(path, organ + '_label_slices_' + str(resolution) + '.pt'))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        # Adding channel dimension.\n",
    "        return image[None, :], label[None, :]\n",
    "\n",
    "    def show_datapoint(self, index):\n",
    "        image, label = self.__getitem__(index)\n",
    "        superimpose(image, label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And create a function, which will split a dataset and create appropriate dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loaders(data, batch_size):\n",
    "    N = len(data); N_t = int(0.9*N); N_d = N - N_t\n",
    "    train_data, dev_data = D.random_split(data, [N_t, N_d])\n",
    "    train_loader = D.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    dev_loader = D.DataLoader(dev_data, batch_size=batch_size, shuffle=True)\n",
    "    return train_loader, dev_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check if the dataset functions directly by retrieving and displaying a single datapoint. One must specify the organ and the resolution of the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Defining our model architecture\n",
    "We define the variational encoder architecture as a pytorch module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        super(Conv, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "    \n",
    "class ConvTranspose(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        super(ConvTranspose, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class VAEModel(nn.Module):\n",
    "    def __init__(self, base):\n",
    "        super(VAEModel, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            Conv(1, base, 3, stride=2, padding=1),\n",
    "            Conv(base, 2*base, 3, padding=1),\n",
    "            Conv(2*base, 2*base, 3, stride=2, padding=1),\n",
    "            Conv(2*base, 2*base, 3, padding=1),\n",
    "            Conv(2*base, 2*base, 3, stride=2, padding=1),\n",
    "            Conv(2*base, 4*base, 3, padding=1),\n",
    "            Conv(4*base, 4*base, 3, stride=2, padding=1),\n",
    "            Conv(4*base, 4*base, 3, padding=1),\n",
    "            Conv(4*base, 4*base, 3, stride=2, padding=1),\n",
    "            nn.Conv2d(4*base, 64*base, 8),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.encoder_mu = nn.Conv2d(64*base, 32*base, 1)\n",
    "        self.encoder_logvar = nn.Conv2d(64*base, 32*base, 1)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(32*base, 64*base, 1),\n",
    "            ConvTranspose(64*base, 4*base, 8),\n",
    "            Conv(4*base, 4*base, 3, padding=1),\n",
    "            ConvTranspose(4*base, 4*base, 4, stride=2, padding=1),\n",
    "            Conv(4*base, 4*base, 3, padding=1),\n",
    "            ConvTranspose(4*base, 4*base, 4, stride=2, padding=1),\n",
    "            Conv(4*base, 2*base, 3, padding=1),\n",
    "            ConvTranspose(2*base, 2*base, 4, stride=2, padding=1),\n",
    "            Conv(2*base, 2*base, 3, padding=1),\n",
    "            ConvTranspose(2*base, 2*base, 4, stride=2, padding=1),\n",
    "            Conv(2*base, base, 3, padding=1),\n",
    "            ConvTranspose(base, base, 4, stride=2, padding=1),\n",
    "            nn.Conv2d(base, 1, 3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.ConvTranspose2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return self.encoder_mu(x), self.encoder_logvar(x)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "    def forward_latent(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setting up a training environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a custom loss function, which combines binary-cross-entropy loss and Kullbackâ€“Leibler divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(batch, recon, mu, log_var):\n",
    "    recon_loss = nn.BCELoss(reduction='mean')\n",
    "    kld = (-0.5 * torch.mean(1 + log_var - mu.pow(2) - log_var.exp()))\n",
    "    return recon_loss(recon, batch) + 0.1 * kld"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a training routine for a single epoch, i.e. a full round of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, train_loader):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    for batch in train_loader:\n",
    "        x = batch[0].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        x_hat, mu, log_var = model(x)\n",
    "        loss = loss_fn(x, x_hat, mu, log_var)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item()   \n",
    "    return losses / len(train_loader)  # average loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And an evaluation routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dev_loader):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    for data in dev_loader:\n",
    "        x = data[0].to(device)\n",
    "        x_hat, mu, log_var = model.forward(x)\n",
    "        loss = loss_fn(x, x_hat, mu, log_var)\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(dev_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Putting it all together  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an instance of the CT_Dataset, specifying the organ, on which we wish to train our model. This set is passed onto the dataloaders. Be careful with running this, as the dataset variable will take up a lot of space in RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = 2**8\n",
    "batch_size = 32;\n",
    "dataset = CT_Dataset(prep_data_dir, 'lung', resolution)\n",
    "train_loader, dev_loader = make_loaders(dataset, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We inspect a datapoint directly and from one of the dataloaders to make sure everything works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = dataset.__getitem__(1)\n",
    "print(np.shape(image))\n",
    "batch = next(iter(train_loader))\n",
    "print(np.shape(batch[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use our previously defined class to create an instance of our model. We specify the optimizer, as well as a learning rate scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAEModel(base=16); model = model.to(device)\n",
    "lr = 4e-3\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epochs = 70\n",
    "use_checkpoint = True; record_checkpoint = True\n",
    "\n",
    "train_losses = []; dev_losses = []; lrs = []\n",
    "\n",
    "# Load last checkpoint, if present.\n",
    "if use_checkpoint and len(list_checkpoints(checkpoint_dir)) != 0:\n",
    "    model = load_model(checkpoint_dir, model)\n",
    "    optimizer = load_optimizer(checkpoint_dir, optimizer)\n",
    "    trained_epochs = max(list_checkpoints(checkpoint_dir))\n",
    "    print('Training from epoch ' + str(trained_epochs))\n",
    "else:\n",
    "    print('Training from scratch:')\n",
    "    trained_epochs = 0;\n",
    "\n",
    "try:\n",
    "    for epoch in range(trained_epochs+1, total_epochs+1):\n",
    "        lrs.append(optimizer.param_groups[0]['lr'])\n",
    "        start_time = timer()\n",
    "        train_loss = train_epoch(model, optimizer, train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        end_time = timer()\n",
    "        dev_loss = evaluate(model, dev_loader)\n",
    "        dev_losses.append(dev_loss)\n",
    "        scheduler.step()\n",
    "\n",
    "        trained_epochs = epoch\n",
    "        \n",
    "        print((f\"Epoch {epoch}:, Train-loss: {train_loss:.4f}, Dev-loss: {dev_loss:.4f}, \"f\"Epoch-time = {(end_time - start_time):.3f}s\"))\n",
    "\n",
    "        if epoch % 5 == 0 and record_checkpoint:\n",
    "            save_checkpoint(checkpoint_dir, trained_epochs, model, optimizer)\n",
    "        \n",
    "    print('Training completed.')  \n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('Manually stopped.')\n",
    "    save_checkpoint(checkpoint_dir, trained_epochs, model, optimizer)\n",
    "\n",
    "print('Checkpoint saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the trained VAE model along with loss metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '../saved_models/' + 'vae_model.pt')\n",
    "torch.save(train_losses, '../losses/' + 'vae_train_losses.pt')\n",
    "torch.save(dev_losses, '../losses/' + 'vae_dev_losses.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check if the VAE can reconstruct images, only from the feature vector z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # Tell model to not update weights.\n",
    "\n",
    "original = next(iter(dev_loader))[0][0].view(1, 1, 256, 256).to(device)\n",
    "reconstruction = model.forward(original)[0]\n",
    "\n",
    "draw(original.cpu().view(1,256,256), \n",
    "     reconstruction.cpu().view(1,256,256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the loss metrics for the last 45 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(torch.load(join(losses_dir, 'vae_train_losses.pt'))))\n",
    "plt.plot(np.array(torch.load(join(losses_dir, 'vae_dev_losses.pt'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationModel(nn.Module):\n",
    "    def __init__(self, base):\n",
    "        super(SegmentationModel, self).__init__()\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(32*base, 64*base, 1),\n",
    "            ConvTranspose(64*base, 4*base, 8),\n",
    "            Conv(4*base, 4*base, 3, padding=1),\n",
    "            ConvTranspose(4*base, 4*base, 4, stride=2, padding=1),\n",
    "            Conv(4*base, 4*base, 3, padding=1),\n",
    "            ConvTranspose(4*base, 4*base, 4, stride=2, padding=1),\n",
    "            Conv(4*base, 2*base, 3, padding=1),\n",
    "            ConvTranspose(2*base, 2*base, 4, stride=2, padding=1),\n",
    "            Conv(2*base, 2*base, 3, padding=1),\n",
    "            ConvTranspose(2*base, 2*base, 4, stride=2, padding=1),\n",
    "            Conv(2*base, base, 3, padding=1),\n",
    "            ConvTranspose(base, base, 4, stride=2, padding=1),\n",
    "            nn.Conv2d(base, 1, 3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.ConvTranspose2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.decoder(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce_loss = nn.BCELoss(reduction='mean')\n",
    "\n",
    "def train_epoch(vae_model, seg_model, optimizer, train_loader):\n",
    "    seg_model.train()\n",
    "    vae_model.eval()\n",
    "    losses = 0\n",
    "    for batch in train_loader:\n",
    "        x = batch[0].to(device)\n",
    "        y = batch[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        z = vae_model.forward_latent(x)\n",
    "        y_hat = seg_model.forward(z)\n",
    "        loss = bce_loss(y, y_hat)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item()   \n",
    "    return losses / len(train_loader)  # average loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(vae_model, seg_model, dev_loader):\n",
    "    seg_model.eval()\n",
    "    vae_model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    for data in dev_loader:\n",
    "        x = data[0].to(device)\n",
    "        y = batch[1].to(device)\n",
    "        x_hat, mu, log_var = model.forward(x)\n",
    "        z = vae_model.forward_latent(x)\n",
    "        y_hat = seg_model.forward(z)\n",
    "        loss = bce_loss(y, y_hat)\n",
    "        losses += loss.item() \n",
    "    return losses / len(dev_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_seg_loaders(data, batch_size):\n",
    "    N = len(data); N_train = int(0.8*N); \n",
    "    N_dev = int((N - N_train)/2); N_test = int(N - N_train - N_dev)\n",
    "    train_data, dev_data, test_data = D.random_split(data, [N_train, N_dev, N_test])\n",
    "    train_loader = D.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    dev_loader = D.DataLoader(dev_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = D.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "    return train_loader, dev_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_loader, dev_loader, test_loader \u001b[39m=\u001b[39m make_seg_loaders(dataset, batch_size)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "resolution = 2**8\n",
    "batch_size = 32;\n",
    "dataset = CT_Dataset(prep_data_dir, 'lung', resolution)\n",
    "train_loader, dev_loader, test_loader = make_seg_loaders(dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_model = torch.load('../saved_models/vae_model.pt').to(device)\n",
    "seg_model = SegmentationModel(base=16).to(device)\n",
    "lr = 4e-3\n",
    "optimizer = optim.AdamW(seg_model.parameters(), lr=lr)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epochs = 50\n",
    "\n",
    "train_losses = []; dev_losses = []; lrs = []\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    lrs.append(optimizer.param_groups[0]['lr'])\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(vae_model, seg_model, optimizer, train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    end_time = timer()\n",
    "    dev_loss = evaluate(vae_model, seg_model, dev_loader)\n",
    "    dev_losses.append(dev_loss)\n",
    "    scheduler.step()\n",
    "    print((f\"Epoch {epoch}:, Train-loss: {train_loss:.4f}, Dev-loss: {dev_loss:.4f}, \"f\"Epoch-time = {(end_time - start_time):.3f}s\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''' Segmentation evaluation '''\n",
    "def IoU(label, recon, thresh):\n",
    "    inter = ((label >= thresh) & (recon >= thresh)) * 1.0\n",
    "    union = ((label >= thresh) | (recon >= thresh)) * 1.0\n",
    "    return inter.sum() / union.sum() / label.shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4774e11bc94230fbd552f0cedba07848c67b493bb83ff5140fd93cc0e9d8c643"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
