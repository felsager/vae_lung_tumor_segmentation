{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation of Medical Scans using Variational VAE's\n",
    "This notebook goes through the process of creating a pytorch-compatible dataset, and setting up a model for segmentation of tumors in various organs. It enables reproduceability of our final model and testing results.\n",
    "\n",
    "## 1. Setup\n",
    "We import some necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For ML\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as D\n",
    "import torch.optim as optim\n",
    "from torch import Tensor\n",
    "import torchvision.transforms as Transform\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# For reading raw data.\n",
    "import json\n",
    "import nibabel as nib\n",
    "\n",
    "# For displaying and evaluating results.\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# For monitoring resource-usage and progress.\n",
    "from timeit import default_timer as timer\n",
    "from tqdm import tqdm # Install ipywidgets to remove warning.\n",
    "import os, sys, psutil\n",
    "from os.path import join, exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And check if our GPU is available, while also retrieving some system stats. We need a lot of RAM, because our selected datasets are very large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n",
      "NVIDIA GeForce GTX 1070\n",
      "CUDA version: 11.7\n",
      "RAM: 16.74GB\n"
     ]
    }
   ],
   "source": [
    "# Setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using', device)\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('CUDA version:', torch.version.cuda)\n",
    "\n",
    "available_ram = round(psutil.virtual_memory()[0]/1000000000,2)\n",
    "print('RAM: ' + str(available_ram) + 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We setup up some global constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '../' # Location of project, relative to the working directory.\n",
    "raw_data_dir = join(root_dir, 'raw_data')\n",
    "prep_data_dir = join(root_dir, 'augmenteddata')\n",
    "\n",
    "cmap_seg = ListedColormap(['none', 'red']) # For drawing tumors in red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And some utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def superimpose(image, label):\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.imshow(label, cmap=cmap_seg)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We preprocess and format datasets from raw data for each specified organ, using the above function. We save progress after each organ is completed. Can be interrupted and resumed at any time, and accounts for progress, which has already been made. We define a function which loads and stores our data in the proper formatting. As the datasets are huge and have to concatenate each set of 240 slices to the previous, we monitor progress and RAM-usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(organ, type, resolution):\n",
    "    with open(join(raw_data_dir, organ, 'dataset.json')) as f:\n",
    "        manifest = json.load(f)['training']\n",
    "    \n",
    "    bar = tqdm(total=len(manifest))\n",
    "    bar.set_description('Prepping ' + organ + ' ' + type + 's')\n",
    "    \n",
    "    resize = Transform.Resize((resolution, resolution))\n",
    "\n",
    "    try: \n",
    "        images = torch.zeros((0, resolution, resolution))\n",
    "\n",
    "        for entry in manifest:\n",
    "            bar.set_postfix(**{'RAM':round(psutil.virtual_memory()[3]/10e8, 2)})\n",
    "            bar.update()\n",
    "\n",
    "            nii_img = nib.load(join(raw_data_dir, organ, entry[type][2:]))\n",
    "\n",
    "            # Convert to numpy array, then pytorch tensor.\n",
    "            nii_data = Tensor(nii_img.get_fdata())\n",
    "\n",
    "            # Scale between 0 and 1.\n",
    "            nii_data -= nii_data.min()\n",
    "            nii_data /= nii_data.max()\n",
    "            nii_data = nii_data.permute(2, 0, 1) # (slice, rows, columns)\n",
    "            nii_data = resize(nii_data)\n",
    "            images = torch.cat((images, nii_data), 0)\n",
    "        \n",
    "        torch.save(images, join(prep_data_dir, organ + '_' + type + '_slices_' + str(resolution) + '.pt'))\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print('Manually stopped.')\n",
    "    \n",
    "    bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call the preprocessor function for the organs, we wish to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already prepped.\n"
     ]
    }
   ],
   "source": [
    "lod = 2**7                 # Level of detail.\n",
    "resolution = lod           # 2**8 = 256\n",
    "do_prep = False            # Toggle to prep data.\n",
    "\n",
    "organs = ['spleen','colon','pancreas','lung','liver']\n",
    "\n",
    "if do_prep:\n",
    "    for organ in organs:\n",
    "        prep_data(organ,'image',resolution)\n",
    "        prep_data(organ,'label',resolution)\n",
    "else:\n",
    "    print('Data already prepped.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating a pytorch dataset\n",
    "We define a custom dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CT_Dataset(Dataset):\n",
    "    def __init__(self, path, organ, resolution):\n",
    "        self.images = torch.load(join(path, organ + '_image_slices_' + str(resolution) + '.pt'))\n",
    "        \n",
    "        self.labels = torch.load(join(path, organ + '_label_slices_' + str(resolution) + '.pt'))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.images[index], self.labels[index]\n",
    "\n",
    "    def show_datapoint(self, index):\n",
    "        image, label = self.__getitem__(index)\n",
    "        superimpose(image, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check if the dataset functions directly by retrieving and displaying a single datapoint. One must specify the organ and the resolution of the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Defining our model architecture\n",
    "We define the variational encoder architecture as a pytorch module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_size = 1024 # latent space size\n",
    "w = h = 256 # output of encoder size\n",
    "\n",
    "class VAEModel(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(VAEModel, self).__init__()\n",
    "        self.activation = nn.LeakyReLU(0.05)\n",
    "        self.encoder = nn.Sequential()\n",
    "        for i in range(0, 6):\n",
    "            self.encoder.append(nn.Sequential(\n",
    "                nn.Conv2d(2**i, 2**(i+1), 4, 2, 1),\n",
    "                self.activation,))\n",
    "\n",
    "        self.decoder = nn.Sequential()\n",
    "        for i in range(2, 7):\n",
    "            self.decoder.append(nn.Sequential(\n",
    "                nn.ConvTranspose2d(2**(8-i), 2**(7-i), 4, 2, 1),\n",
    "                self.activation,))\n",
    "        self.decoder.append(nn.Sequential(\n",
    "            nn.ConvTranspose2d(2, 1, kernel_size = 4, stride = 2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "            )\n",
    "        )\n",
    "\n",
    "        '''\n",
    "        self.segmentation_decoder = nn.Sequential()\n",
    "        for i in range(0, 7):\n",
    "            self.segmentation_decoder.append(nn.Sequential(\n",
    "                nn.ConvTranspose2d(2**(8-i), 2**(7-i), 4, 2, 1),\n",
    "                self.activation,))\n",
    "        self.segmentation_decoder.append(nn.Sequential(\n",
    "            nn.ConvTranspose2d(2, 1, kernel_size = 4, stride = 2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "            )\n",
    "        )'''\n",
    "\n",
    "        self.fc_mu = nn.Linear(1024, latent_size)\n",
    "        self.fc_log_sigma = nn.Linear(1024, latent_size)\n",
    "        self.latent_de = nn.Linear(1024, 1024)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.ConvTranspose2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "    \n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        h = h.view(-1, 1024)\n",
    "        h = self.activation(h)\n",
    "        mu = self.fc_mu(h)\n",
    "        log_sigma = self.fc_log_sigma(h)\n",
    "        return mu, log_sigma\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = self.latent_de(z)\n",
    "        z = self.activation(z)\n",
    "        z = z.view(-1, 64, 4, 4)\n",
    "        z = self.decoder(z)\n",
    "        return z\n",
    "\n",
    "    def reparameterize(self, mu, log_sigma):\n",
    "        std = torch.exp(0.5*log_sigma)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, w, h) # add channel dimension\n",
    "        mu, log_sigma = self.encode(x) # log sigma is more stable (numerically)\n",
    "        z = self.reparameterize(mu, log_sigma)\n",
    "        x_hat = self.decode(z)\n",
    "        return x_hat.view(-1, w, h), mu, log_sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a custom loss function, which combines binary-cross-entropy loss and Kullbackâ€“Leibler divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce_loss = nn.BCELoss(reduction='mean')\n",
    "\n",
    "def loss_fn(x, x_hat, mu, log_var): # Input, reconstructed input.\n",
    "    x = x.reshape(x_hat.shape)\n",
    "    BCE = bce_loss(x_hat, x)\n",
    "    KLD = -0.5*torch.sum(1+log_var-mu.pow(2)-log_var.exp()) # KL divergence.\n",
    "    KLD = KLD.mean()       # Average over batch.\n",
    "    loss = BCE + 1e-6*KLD  # Beta_norm * KLD.\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to make and return data-loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loaders(data, batch_size):\n",
    "    N = len(data); N_t = int(0.9*N); N_d = N - N_t\n",
    "    train_data, dev_data = D.random_split(data, [N_t, N_d])\n",
    "    train_loader = D.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    dev_loader = D.DataLoader(dev_data, batch_size=batch_size, shuffle=True)\n",
    "    return train_loader, dev_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a training routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, train_loader):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    for data in train_loader:\n",
    "        x = data[0].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        x_hat, mu, log_var = model.forward(x)\n",
    "        loss = loss_fn(x, x_hat, mu, log_var)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item()    \n",
    "    return losses / len(train_loader)  # average loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And an evaluation routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dev_loader):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    for data in dev_loader:\n",
    "        x = data[0].to(device)\n",
    "        x_hat, mu, log_var = model.forward(x)\n",
    "        loss = loss_fn(x, x_hat, mu, log_var)\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(dev_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use our previously defined class to create an instance of our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAEModel(); model = model.to(device)\n",
    "batch_size = 64; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an instance of the CT_Dataset, specifying the organ, on which we wish to train our model. Be careful with running this, as the dataset variable will take up a lot of space in RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = 2**8\n",
    "dataset = CT_Dataset(prep_data_dir, 'lung', resolution)\n",
    "train_loader, dev_loader = make_loaders(dataset, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We inspect a datapoint from our set to make sure it works properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.show_datapoint(900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specify the optimizer, we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-3\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.8)\n",
    "\n",
    "train_losses = []; dev_losses = []; lrs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 500\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    lrs.append(optimizer.param_groups[0]['lr'])\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(model, optimizer, train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    end_time = timer()\n",
    "    dev_loss = evaluate(model, dev_loader)\n",
    "    dev_losses.append(dev_loss)\n",
    "    scheduler.step()\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.4f}, Dev loss: {dev_loss:.4f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_no = 49\n",
    "torch.save(model, f'../saved_models/model_{model_no}')\n",
    "torch.save(torch.Tensor(train_losses), f'../losses/train_losses_model_{model_no}')\n",
    "torch.save(torch.Tensor(dev_losses), f'../losses/dev_losses_model_{model_no}')\n",
    "dev_losses_2 = torch.load(f'../losses/dev_losses_model_{44}')\n",
    "print(dev_losses_2[-1], dev_losses[-1])\n",
    "#print(len(dev_losses_2), len(dev_losses))\n",
    "#plt.plot((dev_losses_2))\n",
    "plt.plot((dev_losses))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw(x, x_hat):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(8,5))\n",
    "    img_0 = x[0].detach().numpy()\n",
    "    img_1 = x_hat[0].detach().numpy()\n",
    "    #img = img.reshape((-1, 28, 28)).transpose((1, 0, 2)).reshape(-1, 10*28)\n",
    "    #img = img * 0.3081 + 0.1307\n",
    "    axs[0].imshow(img_0, vmin=0, vmax=1, cmap='gray')\n",
    "    axs[1].imshow(img_1, vmin=0, vmax=1, cmap='gray')\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "x_test = next(iter(dev_loader))[0][0].view(1, 256, 256).to(device)\n",
    "#x_test = torch.zeros((1, 256, 256)).to(device)\n",
    "x_hat_test = model.forward(x_test)[0]\n",
    "draw(x_test.cpu(), x_hat_test.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "x_sampled = model.decode(torch.randn(1, latent_size).to(device)).view(1, 256, 256).cpu()\n",
    "plt.imshow(x_sampled[0].detach().numpy(), vmin=0, vmax=1, cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('dl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4b9f82d0d752eb34e678ed91836faafc818af014405bd67d0435880e47f8c111"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
