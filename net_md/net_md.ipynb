{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation of Medical Scans using Variational VAE's\n",
    "This notebook goes through the process of creating a pytorch-compatible dataset, and setting up a model for segmentation of tumors in various organs. It enables reproduceability of our final model and testing results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "We import some necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For ML\n",
    "import torch\n",
    "\n",
    "# For reading raw data.\n",
    "import json\n",
    "import nibabel as nib\n",
    "\n",
    "# For displaying and evaluating results.\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# For monitoring resource-usage and progress.\n",
    "from tqdm import tqdm\n",
    "import os, sys, psutil\n",
    "from os.path import join, exists\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if GPU is available and retrieve some system stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n",
      "NVIDIA GeForce GTX 1070\n",
      "CUDA version: 11.7\n",
      "RAM: 16.74GB\n"
     ]
    }
   ],
   "source": [
    "# Setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using', device)\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('CUDA version:', torch.version.cuda)\n",
    "\n",
    "available_ram = round(psutil.virtual_memory()[0]/1000000000,2)\n",
    "print('RAM: ' + str(available_ram) + 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making sure, we are in the correct working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/nv/Storage/Data-Science/vae_lung_tumor_segmentation/net_md\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up some global constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = './' # Location of project, relative to the working directory.\n",
    "raw_data_dir = join(root_dir, 'data', 'raw_data')\n",
    "aug_data_dir = join(root_dir, 'data', 'aug_data')\n",
    "model_dir = join(root_dir, 'model')\n",
    "stats_dir = join(root_dir, 'stats')\n",
    "output_dir = join(root_dir, 'output')\n",
    "\n",
    "organs = ['spleen', 'colon', 'lung']\n",
    "\n",
    "d = 256 # New dimensions (width and height) of datapoints.\n",
    "num_chunks = 16 # Number of chunks to divide our data into."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to load current progress, as to not repeat work, which has already been completed, when running the entire notebook at once. The progress flags are stored externally in `progression.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_progress():\n",
    "    with open('progress.json','r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def set_progress(progress):\n",
    "    with open('progress.json', \"w\") as f: \n",
    "        json.dump(progress, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We setup a folder structure for our data - both raw and preprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not exists(aug_data_dir): os.makedirs(aug_data_dir)\n",
    "if not exists(output_dir): os.makedirs(aug_data_dir)\n",
    "if not exists(model_dir): os.makedirs(model_dir)\n",
    "\n",
    "if not exists(raw_data_dir):\n",
    "    os.mkdir(raw_data_dir)\n",
    "    for organ in organs:\n",
    "        os.mkdir(os.path.join(raw_data_dir, organ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `raw_data/` sub-directories for each organ has to be populated manually using the unzipped files from [medicaldecathlon.com](https://drive.google.com/drive/folders/1HqEgzS8BV2c7xYNrZdEAnrHk7osJJ--2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Inspecting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import gzip\n",
    "\n",
    "organ = 'lung'\n",
    "organ_path = join(raw_data_dir, organ)\n",
    "file = join(organ_path, 'dataset.json')\n",
    "with open(file,'r') as f:\n",
    "     manifest = json.loads(f.read())\n",
    "\n",
    "train_manifest = manifest['training']\n",
    "test_manifest = manifest['test']\n",
    "\n",
    "image_path = train_manifest[1]['image'][2:]\n",
    "label_path = train_manifest[1]['label'][2:]\n",
    "\n",
    "with gzip.open(join(organ_path, image_path),'rb') as f:\n",
    "    decomp_image = f.read()\n",
    "\n",
    "decomp_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function which loads and stores our data in the proper formatting. As the datasets are huge, we monitor progress and RAM-usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torchvision.transforms as Transform\n",
    "\n",
    "resize = Transform.Resize((d, d))\n",
    "rotate = Transform.RandomRotation(180)\n",
    "\n",
    "def prep_data(organ, training_paths, data_tensor, bar):\n",
    "    for _, path in enumerate(training_paths):\n",
    "        bar.set_postfix(**{'RAM':round(psutil.virtual_memory()[3]/10e8, 2)})\n",
    "        bar.update()\n",
    "\n",
    "        # Get path to images - removed dot in path from json-file.\n",
    "        nii_img = nib.load(join(raw_data_dir,organ + path['image'][1:]))\n",
    "        \n",
    "        nii_data = nii_img.get_fdata()\n",
    "        nii_data = Tensor(nii_img.get_fdata())\n",
    "        \n",
    "        # Ensure scale [0; 1]\n",
    "        nii_data -= nii_data.min()\n",
    "        nii_data /= nii_data.max() # Are the max the same in every data point?\n",
    "        nii_data = nii_data.permute(2, 0, 1) # Shape: (slice, rows, columns)\n",
    "        nii_data = resize(nii_data)\n",
    "        data_tensor = torch.cat((data_tensor, nii_data), 0)\n",
    "        \n",
    "    torch.save(data_tensor, join(aug_data_dir,organ+'_slices_unaugmented.pt'))\n",
    "    bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prep and format datasets from raw data for each organ, using the above function. We save progress after each organ is completed. Can be interrupted and resumed at any time, and accounts for progress, which has already been made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress = get_progress()\n",
    "\n",
    "for organ in organs:\n",
    "    if not progress['loaded'][organ]:\n",
    "\n",
    "        path = join(raw_data_dir,organ,'dataset.json')\n",
    "        with open(path) as f:\n",
    "            data_set = json.load(f)\n",
    "        training_paths = data_set['training']\n",
    "\n",
    "        data_tensor = torch.zeros((0, d, d))\n",
    "        total_paths = len(training_paths)\n",
    "\n",
    "        bar = tqdm(total=total_paths)\n",
    "        bar.set_description(f'%s' % organ)\n",
    "\n",
    "        try: \n",
    "            prep_data(organ, training_paths, data_tensor, bar)\n",
    "            print('The', organ, 'was successfully loaded.')\n",
    "\n",
    "            progress['loaded'][organ] = True\n",
    "            set_progress(progress)\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print ('Manually stopped.\\nOrgan:', organ, 'was not saved.')\n",
    "            bar.close()\n",
    "            break\n",
    "    else:\n",
    "        print('The', organ, 'set has already been loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets are so large, that we need to split them into smaller chunks. We first initialize empty chunks, and fill them with augmented data from each organ, evenly split amongst the chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress = get_progress()\n",
    "\n",
    "if not progress['augmented']:\n",
    "    for n in range(num_chunks):\n",
    "        chunk = torch.zeros(0, d, d)\n",
    "        torch.save(chunk, join(aug_data_dir, f'unaugmented_chunk_{n}.pt'))\n",
    "\n",
    "    bar = tqdm(total=len(organs)*num_chunks)\n",
    "    bar.set_description(f'Augmentation')\n",
    "    \n",
    "    try:\n",
    "        for organ in organs:\n",
    "            data = torch.load(join(aug_data_dir,organ+ '_slices_unaugmented.pt'))\n",
    "            N = data.shape[0]\n",
    "            idx = torch.randperm(N)\n",
    "            data = data[idx]\n",
    "            split_idx = int(N/num_chunks)\n",
    "            \n",
    "            for n in range(num_chunks):\n",
    "                path = join(aug_data_dir, f'unaugmented_chunk_{n}.pt')\n",
    "                chunk = torch.load(path)\n",
    "                chunk = torch.cat((chunk, data[n*split_idx:(n+1)*split_idx]), 0)\n",
    "                torch.save(chunk, path)\n",
    "                bar.update()\n",
    "\n",
    "        print('Augmentation was successful.')\n",
    "\n",
    "        # Change state of progression.json\n",
    "        progress['augmented'] = True\n",
    "        set_progress(progress)\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print ('Manually stopped.\\nChunk', str(n), 'was not saved.')\n",
    "        bar.close()\n",
    "        \n",
    "else:\n",
    "    print('Data augmentation and chunking already completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We inspect the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_data = torch.load(os.path.join(aug_data_dir,'unaugmented_chunk_1.pt'))\n",
    "np.shape(some_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Management\n",
    "We define a custom dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import gzip\n",
    "\n",
    "class CT_Dataset(Dataset):\n",
    "    def __init__(self, lod=9):\n",
    "        assert(lod < 10)\n",
    "        self.data = {}\n",
    "        self.current_lod = lod\n",
    "\n",
    "        for lvl in range(lod + 1):\n",
    "            x = self.load(lvl, 'scans', np.float16, factor=1.0)\n",
    "            t = self.load(lvl, 'labels', np.uint8, factor=(1.0 / 255.0))\n",
    "            self.data['lod_{}'.format(lvl)] = {'x': x, 't': t}\n",
    "\n",
    "    def load(self, lvl, prefix, dtype, factor):\n",
    "        resolution = 2**lvl\n",
    "        path = join(raw_data_dir,'_data_{}_res_{}.gz'.format(prefix, resolution))\n",
    "        \n",
    "        with gzip.open(path, 'rb') as f:\n",
    "            data = f.read()\n",
    "        \n",
    "        data = np.frombuffer(data, dtype=dtype)\n",
    "        samples = int(data.shape[0] / resolution**2)\n",
    "        data = data.reshape((samples, resolution, resolution))\n",
    "        data = np.expand_dims((data * factor).astype(np.float32), axis=1)\n",
    "        return data\n",
    "\n",
    "    def set_lod(self, lod):\n",
    "        assert(lod < 10)\n",
    "        self.current_lod = lod\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data['lod_0']['x'])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        return {'x': self.data['lod_{}'.format(self.current_lod)]['x'][idx],\n",
    "                't': self.data['lod_{}'.format(self.current_lod)]['t'][idx]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Creating Model Architecture\n",
    "We first import the necessary architecture-components from pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module, ModuleList, Conv2d, ConvTranspose2d, Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our model architecture for the segmenting variational autoencoder, which consists of an encoder and two separate decoders, so it makes sense to define each component as their own class, i.e. pytorch module. We start with the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Module):\n",
    "    def __init__(self, shape, initial):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.shape = shape\n",
    "        self.initial = initial\n",
    "\n",
    "        self.downsample_block = ModuleList()\n",
    "        self.mu_encoding_block = ModuleList()\n",
    "        self.var_encoding_block = ModuleList()\n",
    "\n",
    "        for level in range(int(np.log2(self.shape[1])-1)):\n",
    "            if level == 0:\n",
    "                self.downsample_block.append(\n",
    "                    Conv2d(in_channels=self.shape[0], out_channels=initial, kernel_size=4, stride=2, padding=1))\n",
    "            else:\n",
    "                self.downsample_block.append(\n",
    "                    Conv2d(in_channels=initial*2**(level-1), out_channels=initial*2**(level), kernel_size=4, stride=2, padding=1))\n",
    "\n",
    "            features, length = self.initial*2**(level+2), int(2**(level+2))\n",
    "            \n",
    "            self.mu_encoding_block.append(\n",
    "                Linear(in_features=features, out_features=length))\n",
    "            \n",
    "            self.var_encoding_block.append(\n",
    "                Linear(in_features=features, out_features=length))\n",
    "    \n",
    "    def encode(self, x, scale):\n",
    "        for layer in range(scale):\n",
    "            x = self.downsample_block[layer](x)\n",
    "            \n",
    "        x = self.downsample_block[scale](x)\n",
    "        shape = x.shape\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        mu = (self.mu_encoding_block[scale](x))\n",
    "        log_var = (self.var_encoding_block[scale](x))\n",
    "        \n",
    "        return mu, log_var, shape\n",
    "\n",
    "    def sample(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var) # Reparameterization.\n",
    "        epsilon = torch.randn_like(std)\n",
    "        return mu + std * epsilon # z\n",
    "\n",
    "    def forward(self, x, scale):\n",
    "        mu, log_var, shape = self.encode(x, scale)\n",
    "        z = self.sample(mu, log_var)\n",
    "        return z, mu, log_var, shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then the decoder architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Module):\n",
    "    def __init__(self, shape, initial, act):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.act = act\n",
    "        self.shape = shape\n",
    "        self.initial = initial\n",
    "\n",
    "        self.conv_upsample_block = ModuleList()\n",
    "        self.z_decoding_block = ModuleList()\n",
    "\n",
    "        for level in range(int(np.log2(self.shape[1])-1)):\n",
    "            if level == 0:\n",
    "                self.conv_upsample_block.append(\n",
    "                    ConvTranspose2d(in_channels=initial, out_channels=self.shape[0], kernel_size=4, stride=2, padding=1))\n",
    "            else:\n",
    "                self.conv_upsample_block.append(\n",
    "                    ConvTranspose2d(in_channels=int(initial*2**(level)), out_channels=int(initial*2**(level-1)), kernel_size=4, stride=2, padding=1))\n",
    "\n",
    "            features, length = self.initial*2**(level+2), int(2**(level+2))\n",
    "\n",
    "            self.z_decoding_block.append(\n",
    "                Linear(in_features=length, out_features=features))\n",
    "    \n",
    "    def decode(self, z, scale, shape):\n",
    "        x = self.act(self.z_decoding_block[scale](z)).reshape(shape)\n",
    "        \n",
    "        for layer in range(scale): # Transposed convolutions.\n",
    "            x = self.act(self.conv_upsample_block[scale-layer](x))\n",
    "        x = self.conv_upsample_block[0](x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, z, scale, shape, detach=False):\n",
    "        if detach: z = z.detach()\n",
    "        x = self.decode(z, scale, shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use these two in combination to create our variational autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(Module):\n",
    "    def __init__(self, shape, initial, act):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(shape, initial)\n",
    "        self.decoder = Decoder(shape, initial, act)\n",
    "        self.segmenter = Decoder(shape, initial, act)\n",
    "\n",
    "    def forward(self, x, lod, print=False):\n",
    "        lod = lod - 2\n",
    "        z, mu, log_var, shape = self.encoder.forward(x, lod)\n",
    "        \n",
    "        x_reconst = torch.sigmoid(\n",
    "            self.decoder.forward(z, lod, shape))\n",
    "        \n",
    "        x_segment = torch.sigmoid(\n",
    "            self.segmenter.forward(z, lod, shape, detach=True))\n",
    "\n",
    "        if print: print(z)\n",
    "        \n",
    "        return ((x_reconst, mu, log_var), x_segment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Defining Training- and Evaluation Routines\n",
    "We define a class, which contain our testing routine, checkpoint management, evaluation routine, and some utility functions. This will allow us to easily run tests with different hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test():\n",
    "    def __init__(self, model, dir, load, optimizer, criterions, iou_thresh):\n",
    "        self.model = model\n",
    "        self.dir = dir\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.criterions = criterions\n",
    "        self.io_thresh = iou_thresh\n",
    "        \n",
    "        if load: \n",
    "            self.load_checkpoint(os.path.join(self.dir, 'checkpoint.pt'))\n",
    "        \n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def train(self, dataloader, epochs, lod, print_at=10):\n",
    "        for epoch in range(epochs):\n",
    "            batch_count = 0\n",
    "            losses = np.array([0] * len(self.criterions), dtype=np.float32)\n",
    "\n",
    "            for batch in dataloader:\n",
    "                print(np.shape(batch))\n",
    "                y = self.model.forward(batch['x'].to(self.device), lod)\n",
    "\n",
    "                # Backpropagation\n",
    "                self.model.zero_grad()\n",
    "                for i, value in enumerate(zip(y, self.criterions, batch)):\n",
    "                    output, criterion, key = value\n",
    "                    loss = criterion(output, batch[key].to(self.device))\n",
    "                    loss.backward()\n",
    "                    losses[i] += loss\n",
    "\n",
    "                self.optimizer.step()\n",
    "                batch_count += 1\n",
    "\n",
    "            if ((epoch % print_at) == 0):\n",
    "                losses = losses / batch_count\n",
    "                print('Epoch: ' + epoch, 'Reconst/kld loss: ' + losses[0], \n",
    "                      'Seg loss: ' + losses[1])\n",
    "\n",
    "    def IoU(self, label, reconst):\n",
    "        i = ((label >= self.iou_thresh) & (reconst >= self.iou_thresh)) * 1.0\n",
    "        u = ((label >= self.iou_thresh) | (reconst >= self.iou_thresh)) * 1.0\n",
    "        return i.sum() / u.sum() / label.shape[0]\n",
    "\n",
    "    def evaluate(self, dataloader, lod):\n",
    "        batch_count = 0\n",
    "        losses = np.array([0] * len(self.criterions), dtype=np.float32)\n",
    "        iou = 0\n",
    "        for batch in dataloader:\n",
    "            y = self.model.forward(batch['x'].to(self.device), lod)\n",
    "\n",
    "            for i, value in enumerate(zip(y, self.criterions, batch)):\n",
    "                output, criterion, key = value\n",
    "                loss = criterion(output, batch[key].to(self.device))\n",
    "                losses[i] += loss\n",
    "            \n",
    "            iou += self.IoU(batch['t'].to(self.device), y[1])\n",
    "            batch_count += 1\n",
    "        \n",
    "        losses /= batch_count\n",
    "        iou /= batch_count\n",
    "        print('Reconst/kld loss:', losses[0], 'Seg loss:', losses[1], 'IoU:', iou)\n",
    "    \n",
    "    def load_checkpoint(self, path):\n",
    "        cp = torch.load(path)\n",
    "        self.model.load_state_dict(cp['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(cp['optimizer_state_dict'])\n",
    "        \n",
    "        for state in self.optimizer.state.values():\n",
    "            for k, v in state.items():\n",
    "                if torch.is_tensor(v):\n",
    "                    state[k] = v.cuda()\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        path = os.path.join(self.dir, 'checkpoint.pt')\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "        }, path)\n",
    "\n",
    "    def reconstruct(self, dataloader, lod, count):\n",
    "        batch = next(iter(dataloader))\n",
    "        count = min(count, len(batch['x']))\n",
    "        x = batch['x'][:count].to(self.device)\n",
    "        t = batch['t'][:count].to(self.device)\n",
    "        y = self.model.forward(x, lod)\n",
    "        x_reconst = y[0][0]\n",
    "        x_segment = y[1]\n",
    "        return x, t, x_reconst, x_segment\n",
    "\n",
    "    def save_reconst(self, dataloader, lod, count, output_dir):\n",
    "        x_ins, t_ins, x_outs, t_outs = self.reconstruct(dataloader, lod, count)\n",
    "        x_ins = x_ins.detach().cpu().numpy()\n",
    "        t_ins = t_ins.detach().cpu().numpy()\n",
    "        x_outs = x_outs.detach().cpu().numpy()\n",
    "        t_outs = t_outs.detach().cpu().numpy()\n",
    "        \n",
    "        for i, value in enumerate(zip(x_ins, t_ins, x_outs, t_outs)):\n",
    "            x_in, t_in, x_out, t_out = value\n",
    "\n",
    "            x = np.stack([x_in]*3, axis=0).squeeze().transpose((1,2,0))\n",
    "            t = np.stack([t_in]*3, axis=0).squeeze().transpose((1,2,0))\n",
    "            \n",
    "            mask = t[..., 0] > self.iou_thresh\n",
    "            x[mask] = np.array([0, 1, 0.5])*t[mask]\n",
    "            \n",
    "            plt.imsave(join(output_dir, \n",
    "                'input_' + str(i) + 'resolution_' + str(2**lod) + '.png'), x)\n",
    "\n",
    "            x_r = np.stack([x_out]*3, axis=0).squeeze().transpose((1,2,0))\n",
    "            t_r = np.stack([t_out]*3, axis=0).squeeze().transpose((1,2,0))\n",
    "            \n",
    "            mask = t_r[..., 0] > self.iou_thresh\n",
    "            x_r[mask] = np.array([0, 1, 0.5])*t[mask]\n",
    "            \n",
    "            plt.imsave(join(output_dir,\n",
    "                'output_' + str(i) + 'resolution_' + str(2**lod) + '.png'), x_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a custom loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(output, x):\n",
    "    recon_x, mu, log_var = output\n",
    "    batchSize = mu.shape[0]\n",
    "    rl = (recon_x - x).pow(2).sum() / batchSize\n",
    "    kld = -0.5*torch.sum(1+log_var-mu.pow(2)-log_var.exp()) / batchSize\n",
    "    return rl + kld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Putting it all together\n",
    "We pass our custom datasets to pytorch dataloaders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_data = torch.load(os.path.join(aug_data_dir,'unaugmented_chunk_1.pt'))\n",
    "test_data = torch.load(os.path.join(aug_data_dir,'unaugmented_chunk_2.pt'))\n",
    "\n",
    "batchSize = 64\n",
    "num_workers = 2\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_data, batch_size=batchSize,\n",
    "    shuffle=True, num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_data, batch_size=batchSize, \n",
    "    shuffle=True, num_workers=num_workers\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import elu\n",
    "from torch.optim import Adam\n",
    "\n",
    "model = VAE((1, d, d), 16, elu)\n",
    "\n",
    "test = Test(\n",
    "    model=model,\n",
    "    dir=model_dir,\n",
    "    load=False, \n",
    "    optimizer=Adam(model.parameters(), lr=0.001), \n",
    "    criterions=[loss_function, torch.nn.BCELoss()], \n",
    "    iou_thresh=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "lod = 6\n",
    "iterations = 1\n",
    "\n",
    "for i in range(iterations):\n",
    "    test.train(train_loader, epochs, lod, 1)\n",
    "    test.evaluate(test_loader, lod)\n",
    "    test.save_reconst(test_loader, lod, 10, output_dir)\n",
    "    #test.save_checkpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4b9f82d0d752eb34e678ed91836faafc818af014405bd67d0435880e47f8c111"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
